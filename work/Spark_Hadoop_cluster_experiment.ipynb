{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab87e269-e8c4-4c9f-acb9-46872bd95e4e",
   "metadata": {},
   "source": [
    "# <br>**Spark-Hadoop_Local-Docker-Cluster**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88fb16-f833-4c55-990d-9b6f2666ef87",
   "metadata": {},
   "source": [
    "### <br>**1 - Instalação HDFS**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccfe3768-b1bc-49a7-a5a4-61e244132cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hdfs --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f90dc62-ad6c-4686-b9a9-1398fac399dd",
   "metadata": {},
   "source": [
    "### <br>**2 - Imports necessários**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99310086-7442-4a93-9d0d-60ed3f7b128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when, count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca294c7-f6c0-43ac-8078-23cfdb3da6b7",
   "metadata": {},
   "source": [
    "### <br>**3 - Conexão | Criação de diretório | Carregamento de dados no cluster Hadoop**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad13f7c-f44f-4ccd-89db-9ed2db6fac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_client = InsecureClient(\"http://172.25.0.2:9870\", user=\"hadoop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba0f9a9-86f3-4731-99bb-45c840513e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_client.makedirs(\"/user/dfs_data\", permission=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d52bac1b-79f2-489f-967f-e469cd27241a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/user/dfs_data/dataset.parquet'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_client.upload(hdfs_path=\"/user/dfs_data\", local_path=\"dataset.parquet\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5607f-5be9-4337-bf78-6a7b89493d1f",
   "metadata": {},
   "source": [
    "### <br>**4 - Inicia sessão do cluster Spark**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85af6e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.25.0.12:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://172.25.0.8:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-parquet-experiment</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f02877a1310>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-parquet-experiment\").\\\n",
    "        master(\"spark://172.25.0.8:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1g\").\\\n",
    "        getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed6329-82c0-49f7-84b1-955a7b4222fc",
   "metadata": {},
   "source": [
    "### <br>**5 - Carrega dados do Hadoop (HDFS) para o cluster Spark**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7622e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"hdfs://namenode:8020/user/dfs_data/dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c41dc-5cc6-4356-ae0f-a821100bf56e",
   "metadata": {},
   "source": [
    "### <br>**6 - Processamento dos dados na memória (cluster Spark)**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f9dfe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------+-------+---------+---------------+------------------+-----------+----------------+--------------+-------------+\n",
      "|          Timestamp|From Bank|  Account|To Bank|Account.1|Amount Received|Receiving Currency|Amount Paid|Payment Currency|Payment Format|Is Laundering|\n",
      "+-------------------+---------+---------+-------+---------+---------------+------------------+-----------+----------------+--------------+-------------+\n",
      "|2022-08-01 00:15:00|       20|800104D70|     20|800104D70|        8095.07|         US Dollar|    8095.07|       US Dollar|  Reinvestment|            0|\n",
      "|2022-08-01 00:18:00|     3196|800107150|   3196|800107150|        7739.29|         US Dollar|    7739.29|       US Dollar|  Reinvestment|            0|\n",
      "|2022-08-01 00:23:00|     1208|80010E430|   1208|80010E430|        2654.22|         US Dollar|    2654.22|       US Dollar|  Reinvestment|            0|\n",
      "|2022-08-01 00:19:00|     3203|80010EA80|   3203|80010EA80|       13284.41|         US Dollar|   13284.41|       US Dollar|  Reinvestment|            0|\n",
      "|2022-08-01 00:27:00|       20|800104D20|     20|800104D20|           9.72|         US Dollar|       9.72|       US Dollar|  Reinvestment|            0|\n",
      "|2022-08-01 00:29:00|       20|800104D70|     20|800104D70|           5.38|         US Dollar|       5.38|       US Dollar|  Reinvestment|            0|\n",
      "|2022-08-01 00:08:00|     1208|80010E430|   1208|80010E430|           7.66|         US Dollar|       7.66|       US Dollar|  Reinvestment|            0|\n",
      "|2022-08-01 00:29:00|       11|80010E600|     11|80010E600|          16.33|         US Dollar|      16.33|       US Dollar|  Reinvestment|            0|\n",
      "|2022-08-01 00:23:00|     1208|80010E650|   1208|80010E650|           4.86|         US Dollar|       4.86|       US Dollar|  Reinvestment|            0|\n",
      "|2022-08-01 00:15:00|       20|80010E6F0|  27365|8084250A0|         137.72|         US Dollar|     137.72|       US Dollar|   Credit Card|            0|\n",
      "+-------------------+---------+---------+-------+---------+---------------+------------------+-----------+----------------+--------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a844859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: timestamp_ntz (nullable = true)\n",
      " |-- From Bank: integer (nullable = true)\n",
      " |-- Account: string (nullable = true)\n",
      " |-- To Bank: integer (nullable = true)\n",
      " |-- Account.1: string (nullable = true)\n",
      " |-- Amount Received: float (nullable = true)\n",
      " |-- Receiving Currency: string (nullable = true)\n",
      " |-- Amount Paid: float (nullable = true)\n",
      " |-- Payment Currency: string (nullable = true)\n",
      " |-- Payment Format: string (nullable = true)\n",
      " |-- Is Laundering: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85e12008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de linhas: 10000000\n",
      "Numero de colunas: 11\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numero de linhas: {df.count()}\")\n",
    "print(f\"Numero de colunas: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c8412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"From Bank\", \"From_Bank\")\\\n",
    ".withColumnRenamed(\"To Bank\", \"To_Bank\")\\\n",
    ".withColumnRenamed(\"Account.1\", \"Account_1\")\\\n",
    ".withColumnRenamed(\"Amount Received\", \"Amount_Received\")\\\n",
    ".withColumnRenamed(\"Receiving Currency\", \"Receiving_Currency\")\\\n",
    ".withColumnRenamed(\"Amount Paid\", \"Amount_Paid\")\\\n",
    ".withColumnRenamed(\"Payment Currency\", \"Payment_Currency\")\\\n",
    ".withColumnRenamed(\"Payment Format\", \"Payment_Format\")\\\n",
    ".withColumnRenamed(\"Is Laundering\", \"Is_Laundering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b15e4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------+-------+---------+---------------+------------------+-----------+----------------+--------------+-------------+\n",
      "|          Timestamp|From_Bank|  Account|To_Bank|Account_1|Amount_Received|Receiving_Currency|Amount_Paid|Payment_Currency|Payment_Format|Is_Laundering|\n",
      "+-------------------+---------+---------+-------+---------+---------------+------------------+-----------+----------------+--------------+-------------+\n",
      "|2022-08-01 00:15:00|       20|800104D70|     20|800104D70|        8095.07|         US Dollar|    8095.07|       US Dollar|  Reinvestment|            0|\n",
      "|2022-08-01 00:18:00|     3196|800107150|   3196|800107150|        7739.29|         US Dollar|    7739.29|       US Dollar|  Reinvestment|            0|\n",
      "|2022-08-01 00:23:00|     1208|80010E430|   1208|80010E430|        2654.22|         US Dollar|    2654.22|       US Dollar|  Reinvestment|            0|\n",
      "|2022-08-01 00:19:00|     3203|80010EA80|   3203|80010EA80|       13284.41|         US Dollar|   13284.41|       US Dollar|  Reinvestment|            0|\n",
      "|2022-08-01 00:27:00|       20|800104D20|     20|800104D20|           9.72|         US Dollar|       9.72|       US Dollar|  Reinvestment|            0|\n",
      "+-------------------+---------+---------+-------+---------+---------------+------------------+-----------+----------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974fcd2e-4b84-4e4c-9219-f3e6276a2f26",
   "metadata": {},
   "source": [
    "### <br>**7 - Processamento RDD - Resilient Distributed Dataset**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cccc73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_test = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0be72af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f3ca8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df.rdd  # paralelizar dados com o RDD (Resilient Distributed Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28e9a6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[24] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2388b-1d34-4614-a398-e16025f6a978",
   "metadata": {},
   "source": [
    "### <br>**8 - Escrita dos dados em partes para o Hadoop (HDFS)**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "744ef198",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(5)\n",
    "df.write.parquet(\"hdfs://namenode:8020/user/dfs_data/dataset_2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e082ec83-438f-4661-9a6a-deb690bc6fdb",
   "metadata": {},
   "source": [
    "### <br>**9 - Carregamento dos dados escritos para o cluster Spark**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af88e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = spark.read.parquet(\"hdfs://namenode:8020/user/dfs_data/dataset_2.parquet\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e61ad868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------+-------+---------+---------------+------------------+-----------+----------------+--------------+-------------+\n",
      "|          Timestamp|From_Bank|  Account|To_Bank|Account_1|Amount_Received|Receiving_Currency|Amount_Paid|Payment_Currency|Payment_Format|Is_Laundering|\n",
      "+-------------------+---------+---------+-------+---------+---------------+------------------+-----------+----------------+--------------+-------------+\n",
      "|2022-08-01 03:22:00|     3196|802AA8590|   3613|807F26840|         173.37|              Euro|     173.37|            Euro|          Wire|            0|\n",
      "|2022-08-01 03:58:00|    20462|813F5A020|  20462|813F5A020|       13517.51|         US Dollar|   13517.51|       US Dollar|  Reinvestment|            0|\n",
      "|2022-08-01 23:28:00|   155202|839721E60| 155202|839721E60|        2895.31|       Brazil Real|    2895.31|     Brazil Real|  Reinvestment|            0|\n",
      "|2022-08-02 08:31:00|   219871|817CC2FF0| 184776|835106FC0|        2034.14|              Euro|    2034.14|            Euro|        Cheque|            0|\n",
      "|2022-08-02 13:29:00|    14964|8185AF330|  34104|81CDD9DE0|         814.97|         US Dollar|     814.97|       US Dollar|   Credit Card|            0|\n",
      "|2022-08-02 00:40:00|      357|801486710|  85246|82B644B90|       10340.06|              Euro|   10340.06|            Euro|           ACH|            0|\n",
      "|2022-08-03 03:01:00|    14449|835910940| 119972|84252C4F0|          84.05|         US Dollar|      84.05|       US Dollar|        Cheque|            0|\n",
      "|2022-08-01 23:17:00|   336126|81B113150| 336126|81B113150|       10639.19|              Yuan|   10639.19|            Yuan|  Reinvestment|            0|\n",
      "|2022-08-02 06:52:00|   244426|81CAC59A0|1104681|8286D7AE0|          86.86|         US Dollar|      86.86|       US Dollar|        Cheque|            0|\n",
      "|2022-08-02 19:08:00|    11863|810EDCF10|  19300|84D9E6430|         834.59|              Euro|     834.59|            Euro|           ACH|            0|\n",
      "+-------------------+---------+---------+-------+---------+---------------+------------------+-----------+----------------+--------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f5c3b-14f0-4b7b-b6a2-75f53b5499e8",
   "metadata": {},
   "source": [
    "### <br>**10 - Deletar dados do Hadoop (HDFS)**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "406d0cd8-5f67-4a7d-a581-064f82d47fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs_client.delete(\"/user/dfs_data/dataset_2.parquet\", recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d40fef-1ba5-4500-8bfa-d2d4e6271674",
   "metadata": {},
   "source": [
    "### <br>**11 - Parar sessão do Spark**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d601831-d128-4e8b-b31a-b77598e9f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0424a4f-bc4f-41c6-8f58-2e4e6364a8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
